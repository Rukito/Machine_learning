{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Powtórka\n",
    "wyprowadziliśmy metodę uczenia wielowarstwowych sieci nieliniowych bazującą na minimalizacji funkcji kosztu metodą gradientową. W dzisiejszym wykładzie zastanowimy się jak robić to w sposób:\n",
    "* szybszy\n",
    "* zmniejszający ryzyko wpadania w minima lokalne\n",
    "* prowadzący do lepszej generalizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Przyspieszanie uczenia \n",
    "## Bezwładność \n",
    "![Przykładowa ewolucja wag na tle konturu funkcji kosztu. Lewy ślad: brak bezwładności, prawy z włączoną bezwładnością](https://brain.fuw.edu.pl/edu/images/0/08/Bezwładnosc.png)\n",
    "\n",
    "Podobnie jak dla sieci liniowej można uzyskać poprzez dodanie członu bezwładności do formuły zmiany wag:\n",
    "\n",
    "$\\qquad$ $\\Delta w^{(j+1)} = - \\alpha_1 \\frac{\\partial J}{\\partial w} + \\alpha_2\\Delta w^{(j)}  $\n",
    "\n",
    "* Dla prawie płaskiej powierzchni kosztu efektywny współczynnik uczenia jest\n",
    "$ \\frac{1}{1-\\alpha_2}$ razy większy niż w przypadku algorytmu bez bezwładności. \n",
    "\n",
    "* Dla typowych wartości $\\alpha_1 = 0.1$ i $\\alpha_2 = 0.9$ otrzymujemy około 10 krotne przyspieszenie!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaptacyjny dobór parametrów \n",
    "Kiedy obserwujemy proces minimalizacji funkcji błędu to często widać, że stała wartość parametru uczenia $\\alpha_1$ nie jest optymalna. \n",
    "* Czasami widać, że funkcja długi czas maleje prawie monotonicznie - wtedy lepiej byłoby mieć większą wartość  $\\alpha_1$,\n",
    "* kiedy indziej bardzo oscyluje - to świadczy o zbyt dużej wartości $\\alpha_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prosty pomysł \n",
    "Jednym z prostych pomysłów na zautomatyzowanie procesu dobierania wartości $\\alpha_1$ jest następujący schemat zmiany tego parametru:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\Delta \\alpha_1  = \\left\\{ \n",
    "\\begin{array}{lcl}\n",
    "+ a& \\text{dla} & \\Delta J <0 \\\\\n",
    "-b \\alpha_1 & \\text{dla} & \\Delta J >0 \\\\\n",
    "0 & \\text{dla pozostalych przypadkow}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "Po wykonaniu kroku prowadzącego do zwiększenia funkcji kosztu warto go wycofać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elastyczna wsteczna propagacja błędu \n",
    "Resilient Backpropagation — do zmiany wag wykorzystujemy jedynie informację o znaku gradientu. W poniższych formułach $w_q^p$ to waga łącząca wyjście neuronu $q$ z odpowiednim (q-tym) wejściem neuronu $p$.\n",
    "\n",
    "$\\qquad$ $\n",
    "\\Delta w_q^{p(j)}  = - \\text{sign} \\left(\\frac{\\partial J}{\\partial w_p^q}\\right) \\Delta_q^{p(j)} \n",
    "$\n",
    "gdzie: $J$ jest miarą błędu po prezentacji wszystkich bodźców, zaś $\\Delta_q^{p(j)}$:\n",
    "$\\qquad$ $\n",
    "\\Delta_q^{p(j)}  = \\left\\{ \n",
    "\\begin{array}{lcl}\n",
    "\\eta^+ \\Delta_q^{p(j-1)} & \\text{dla} & \\frac{\\partial J^{(j-1)}}{\\partial w_p^q}\\frac{\\partial J^{(j)}}{\\partial w_p^q} >0 \\\\\n",
    "\\eta^- \\Delta_q^{p(j-1)} & \\text{dla} & \\frac{\\partial J^{(j-1)}}{\\partial w_p^q}\\frac{\\partial J^{(j)}}{\\partial w_p^q} <0 \\\\\n",
    "\\Delta_q^{p(j-1)} & \\text{dla pozostalych przypadkow}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "gdzie: $ 0 < \\eta^- < 1 < \\eta^+$. \n",
    "\n",
    "Dodatkowo ustala się ograniczenie na możliwe wartości $ \\Delta_{min} < \\Delta_p^q < \\Delta_{max}$. \n",
    "\n",
    "Standardowo $\\Delta_{min} \\approx 10^{-6}$ a $ \\Delta_{max} \\approx 50$. \n",
    "\n",
    "Dla sigmoidalnych funkcji odpowiedzi metoda ta może poprawić uczenie w obszarze ogonów sigmoidy, gdzie wartość gradientu jest bardzo mała."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metoda najszybszego spadku \n",
    "Kolejnej wartości wagi szukamy wzdłuż prostej wyznaczonej przez poprzedni wektor wag $w^{(j)}$ i kierunek $d^{(j)}$, zmieniając $\\lambda$ tak, aby zminimalizować w danym kierunku $J$:\n",
    "\n",
    "$\\qquad$ $w^{(j+1)} = w^{(j)} + \\lambda d^{(j)}$\n",
    "\n",
    "Kierunek $d$ wybieramy przeciwnie do gradientu $J$\n",
    "\n",
    "$\\qquad$ $ d^{(j)} = -\\nabla J(w^{(j)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zauważmy, że stary i nowy kierunek minimalizacji są ortogonalne, bo $\\lambda$ jest dobrana tak, aby minimalizować funkcję kosztu w kierunku $d^{(j)}$ tzn.:\n",
    "\n",
    "$\\qquad$ $\\frac{\\partial}{\\partial \\lambda} J(w^{(j)} + \\lambda d^{(j)}) = 0 $\n",
    "\n",
    "Ale rozwijając powyższy wzór widzimy, że:\n",
    "\n",
    "\n",
    "$\\qquad$ $\\frac{\\partial}{\\partial \\lambda} J(w^{(j)} + \\lambda d^{(j)}) =  \\frac{\\partial J\\left(w^{(j+1)}\\right)}{\\partial w} \\frac{\\partial (w^{(j)}+ \\lambda d^{(j)})}{\\partial\\lambda}  = \\nabla J(w^{(j+1)}) \\cdot d^{(j)} = -d^{(j+1)} \\cdot d^{(j)} =0\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metoda gradientu sprzężonego\n",
    "Poprzednią metodę można udoskonalić przez rezygnację z ortogonalności kolejnych kroków:\n",
    "\n",
    "$\\qquad$ $d^{(j+1)} = -\\nabla J \\left( w^{(j+1)} \\right) + \\beta d^{(j)}$\n",
    "\n",
    "i trzeba chytrze dobrać $\\beta$ tak, aby jak najmniej psuć efekt osiągnięty w poprzednim kroku. Nowy kierunek szukania powinien więc być taki, aby z dokładnością pierwszego rzędu nie zmieniał składowej gradientu, która w poprzednim kroku została wyzerowana. A zatem chcemy, aby z dokładnością do wyrazów pierwszego stopnia, spełnione było:\n",
    "\n",
    "$\\qquad$ $ d^{(j)} \\cdot \\nabla J\\left( w^{(j)} + \\lambda d^{(j+1)} \\right) = 0 $\n",
    "\n",
    "praktyczny sposób na znalezienie $ \\beta$ spełniającego powyższy warunek podaje reguła Polaka-Ribiere‘a: \n",
    "\n",
    "$\\qquad$ $ \\beta = \\frac{\\left( \\nabla J\\left (w^{(j+1)} \\right)  - \\nabla J \\left( w^{(j)}\\right)\\right) \\cdot \\nabla J\\left(w^{(j+1)}\\right)}{\n",
    "\\left( \\nabla J \\left(w^{(j)}\\right)\\right)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metody quasi-Newtona\n",
    "\n",
    "Oryginalna metoda Newtona polega na minimalizacji funkcji kosztu z wykorzystaniem drugich pochodnych.\n",
    "Rozwijając $J(w)$ wokół bieżącego wektora wag $w_0$ mamy: \n",
    "\n",
    "$\\qquad$ $J(w)=J(w_0)+(w-w_0)\\cdot \\nabla J\\left( w_0\\right) + \\frac{1}{2} (w - w_0) \\cdot H \\cdot(w - w_0) +... $ (∗)\n",
    "\n",
    "gdzie: H to ''hesjan'' (macierz drugich pochodnych cząstkowych $ H_{ij} =\\frac{\\partial^2 J}{\\partial w_i \\partial w_j}$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Różniczkowanie (* * *) daje: \n",
    "\n",
    "$\\qquad$ $ \\nabla J(w) = \\nabla J(w_0) + H \\cdot (w - w_0) + \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "chcemy znaleźć minimum $J$ czyli spełnić warunek $ \\nabla J(w) = 0$:\n",
    "\n",
    "$\\qquad$ $ \\nabla J(w_0) + H \\cdot (w - w_0) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "stąd: \n",
    "\n",
    "$\\qquad$ $w  = w_0 + H^{-1} \\nabla J \\left(w_0 \\right) $\n",
    "\n",
    "Wzór ten można stosować iteracyjnie.\n",
    "\n",
    "Metoda w oryginalnej postaci jest bardzo kosztowna obliczeniowo ($O(n^3)$) i jest niestabilna numerycznie. Stąd też realne implementacje są nieco inne i zasadniczo polegają na iteracyjnej aktualizacji hesjanu. Zwykle wymaga mniej kroków niż metoda gradientów sprzężonych, ale w każdym kroku jest więcej obliczeń i trzeba mieć pamięć na przechowywanie hesjanu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorytm Lavenberg-Marquardt'a\n",
    "Metoda ta korzysta z faktu, że hesjan może być przybliżony przez:\n",
    "\n",
    "$\\qquad$ $ \\mathbf{H} \\approx \\mathbf{J}^T \\mathbf{J}$\n",
    "\n",
    "gdzie: $\\mathbf{J}$ — macierz jakobiego (macierz pierwszych pochodnych cząskowych) natomiast\n",
    "\n",
    "$\\qquad$ $\\nabla J \\left(w^{(j)}\\right) = \\mathbf{J}^T (z^{(j)} - y^{(j)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W metodzie tej wagi zmieniamy:\n",
    "\n",
    "$\\qquad$ $w^{(j+1)} = w^{(j)} + [\\mathbf{J}^T\\mathbf{ J} + \\mu \\mathbf{I}]^{-1} \\mathbf{J}^T (z^{(j)} - y^{(j)})$\n",
    "\n",
    "dla $ \\mu = 0$ jest to metoda Newtona z przybliżoną wartością hesjanu, dla $ \\mu$ dużego metoda dąży do zwykłej metody gradientowej. Metoda Newtona jest szybsza i dokładniejsza w pobliżu minimum $J$.\n",
    "\n",
    "$ \\mu$ jest zmniejszane po każdym udanym kroku a zwiększane jeśli w danym kroku $J$ wzrosło."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Podsumowanie metod przyspieszania uczenia\n",
    "\n",
    "* Ogólnie: Za szybkość płacimy ilością koniecznej pamięci i wymaganą precyzją obliczeń\n",
    "* Algorytm Lavenberg-Marquardt'a: jest najszybszym algorytmem w problemach aproksymacji funkcji dla sieci o średniej wielkości (do kilkuset wag). Prowadzi też, na ogół, w tych problemach do lepszego dopasowania w sensie błędu średniokwadratowego od pozostałych algorytmów. Jest jednak kosztowny jeśli chodzi o zapotrzebowanie na pamięć.\n",
    "* Resilient Backpropagation: wydaje się być najlepszy w problemach rozpoznawania wzorców, ale nie nadaje się w zasadzie do aproksymacji funkcji. Nie jest pamięciożerny.\n",
    "* Algorytm gradientów sprzężonych: jest najbardziej uniwersalnym algorytmem. Ma umiarkowane wymagania co do ilości pamięci.\n",
    "* Zwykła metoda gradientowa: jest najwolniejsza, ale może to być użyteczne jeśli bardziej niż na czasie zależy nam na generalizacji.\n",
    "* Nieliniowość typu ReLu jest odporna na problem wysycania gradientu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem minimów lokalnych \n",
    "Wszystkie metody minimalizacji funkcji kosztu mogą utknąć w minimach lokalnych. Kilka metod, które mogą pomóc zmniejszyć problemy z minimami lokalnymi:\n",
    "* uniknięcie wysycenia sigmoid już na samym początku uczenia. Np. dla unormowanego wejścia i dla sigmoidy z $\\beta = 1 $ wybranie początkowych wag losowych o takich wartościach, że średnie pobudzenie neuronu $a$ jest mniejsze, ale nie za bardzo niż 1 (można losować wagi rzędu $\\sqrt \\frac{1}{k_i}$\tgdzie $k_i$ ilość wejść do jednostki $i$);\n",
    "* poprawianie wag po każdej prezentacji wzorca, przy czym wzorce prezentowane są w losowej kolejności;\n",
    "* zastosowanie jednostek stochastycznych — gradient oraz dodatkowy parametr T temperatura kontrolują prawdopodobieństwo zmiany wagi w określonym kierunku;\n",
    "* delikatne losowe zmiany wag;\n",
    "* przy każdej prezentacji wzorca dodawanie do niego troszkę szumu. Dodanie szumu zawsze spowolni proces uczenia, przy czym mała dawka może pomóc uniknąć minimów lokalnych, duża - znacznie spowalnia uczenie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Twierdzenie o potencjalnych możliwościach sieci\n",
    "Sieć nieliniowa co najmniej dwuwarstwowa może aproksymować dowolną funkcję swoich wejść, ze z góry zadaną dokładnością. Konieczna jest jedynie dostatecznie duża ilość jednostek w warstwach ukrytych. Do aproksymacji dowolnej funkcji ciągłej wystarcza jedna warstwa ukryta. \n",
    "\n",
    "Uzasadnienie nieformalne:\n",
    ">  Każda “rozsądna” funkcja $ F_i{X_k}$ może być przedstawiona jako liniowa kombinacja “wypukłości”, z których każda jest różna od zera tylko w pobliżu $X_k$.\n",
    "> Takie “wypukłości” można skonstruować z dwóch warstw ukrytych. \n",
    "\n",
    "Gdzie jest problem?\n",
    "* twierdzenie mówi jedynie o istnieniu rozwiązania \n",
    "* w ogólności nie wiadomo ile jednostek w warstwach stanowi “dostatecznie dużą ilość”\n",
    "* nie ma gwarancji, że problem da się rozwiązać metodą wstecznej propagacji błędu, tzn. że dotrzemy do minimum globalnego funkcji kosztu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Trywialne twierdzenie: uczenie sieci zawsze się uda jeśli zastosujemy prawidłowy preprocesor.\n",
    "\n",
    "Jeśli w problemie występują symetrie, to o ile to możliwe warto przenieść ich analizę do fazy preprocesingu, bo powodują one powstanie w funkcji kosztu okresowości, wielokrotnych minimów lokalnych, płaskich dolin i wyżyn.\n",
    "\n",
    "Ze standardowych technik przygotowywania danych (nie tylko dla sieci nuropodobnych) warto rozważyć: \n",
    "* wyskalowanie danych, \n",
    "* normalizację danych, \n",
    "* przeprowadzenie analizy składowych głównych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalizacja\n",
    "## O co tu chodzi? \n",
    "![Schemat ilustrujący koncepcję generalizacji](https://brain.fuw.edu.pl/edu/images/f/f7/Generalizacja.png)\n",
    "\n",
    "Na rysunku obok przedstawiona jest schematycznie koncepcja generalizacji. \n",
    "\n",
    "Wyobraźmy sobie, że jest pewna przestrzeń P, która zawiera pary, np. liczb {((a,b), c)}. \n",
    "\n",
    "W tym przykładzie jest to przestrzeń wszystkich odwzorowań $\\mathcal{R}^2 \\rightarrow \\mathcal{R}$. \n",
    "\n",
    "Niektóre z tych par reprezentują pewną konkretną relację R: np. są to pary spełniające warunek $c=\\sqrt{a^2 +b^2}$.\n",
    "\n",
    "Wyobraźmy sobie dalej, że mamy dane dwa skończone zestawy par, które tą relację spełniają, ale oczywiście nie są w stanie obejmować wszystkich możliwych par. \n",
    "\n",
    "Jeden z nich oznaczymy U, a drugi T. Załóżmy, że mamy dwie wersje sieci, które uczymy na zbiorze U (mogą się one różnić architekturą, albo punktem startu procedury uczącej, albo ilością iteracji algorytmu uczącego itp.). \n",
    "\n",
    "Po procesie uczenia sieci te mają mały i porównywalny błąd na zbiorze U, ale jedna z nich nauczyła się relacji wskazanej na rys. jako g1 a druga relacji g2. \n",
    "\n",
    "Na podstawie rezultatów odtwarzania przykładów ze zbioru testowego mówimy, że sieć druga ma lepszą generalizację niż sieć pierwsza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jakie mogą być przyczyny złej generalizacji?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Architektura nie wystarczająca do reprezentacji relacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Architektura zbyt złożona ... \n",
    "... do danego odwzorowania i zbyt długi proces uczenia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dotyczy to sytuacji, gdy zbiór uczący zawiera przykłady danej relacji z szumem. Sieć ma tak dużo parametrów, że jest w stanie nauczyć się szczegółów zbioru uczącego nie związanych z interesującą nas relacją. Zjawisko takie nazywamy ''przeuczeniem''."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uczenie z kryterium wczesnego stopu \n",
    "*  Oprócz zbioru uczącego posiadamy rozłączny z nim zbiór testowy.\n",
    "* W trakcie uczenia obliczamy błąd popełniany przez sieć na zbiorze uczącym i na zbiorze testowym. \n",
    "* W większości przypadków powinniśmy w początkowej fazie uczenia obserwować malenie błędu na obu zbiorach. \n",
    "* Oczekujemy, że w pewnym momencie, kiedy sieć zaczyna się uczyć zbędnych szczegółów to błąd na zbiorze uczącym będzie nadal malał, a na zbiorze testowym zacznie rosnąć. \n",
    "* Uczenie prowadzimy tylko do momentu, kiedy błąd na obu zbiorach maleje. \n",
    "\n",
    "<img src=\"https://brain.fuw.edu.pl/edu/images/7/76/Wczesny_stop.png\", width = 1600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optymalizacja architektury: Regularyzacja\n",
    "> Pomysł: zacznijmy od dużej sieci i po pewnym cyklu uczenia przeanalizujmy połączenia w sieci usuwając mało istotne połączenia lub jednostki. Następnie powtórzmy uczenie.\n",
    "\n",
    "Można spróbować tak zmodyfikować regułę zmiany wag, aby połączenia nieistotne same dążyły do 0; po standardowym kroku uczenia zmniejszamy wagi: \n",
    "\n",
    "$\\qquad$ $ w_q^{p(j+1)} = (1- \\epsilon) w_q^{p(j)} $   (*)\n",
    "\n",
    "Jest to równoważne modyfikacji funkcji kosztu:\n",
    "\n",
    "$\\qquad$ $J = J_0 + \\frac{1}{2} \\gamma \\sum_{pq}\\left(w_q^p \\right)^2$\n",
    "\n",
    "przy $\\epsilon  = \\alpha \\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Konsekwencje:\n",
    "* rozwiązanie jest “gładsze”\n",
    "* wygładzenie funkcji kosztu likwiduje część minimów lokalnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Powyższa funkcja kosztu prowadzi do preferowania większej liczby małych wag zamiast jednej dużej. Sytuację poprawia wyrażenie:\n",
    "\n",
    "$\\qquad$ $ J = J_0 +\\frac{1}{2}\\gamma \\sum_{pq} \\frac{\\left(w_q^p\\right)^2}{1+\\left(w_q^p \\right)^2}$\n",
    "\n",
    "co jest równoważne (*) przy \n",
    "\n",
    "$\\qquad$ $\\epsilon_q^p = \\frac{\\alpha \\gamma}{\\left( 1+ \\left(w_q^p \\right)^2\\right)^2}$. \n",
    "\n",
    "Dzięki temu małe wagi zanikają szybciej niż duże. To załatwia problem zanikania niepotrzebnych połączeń. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aby zautomatyzować usuwanie zbędnych jednostek możemy zastosować:\n",
    "\n",
    "$\\qquad$ $ \\epsilon^p = \\frac{\\alpha \\gamma}{ \\left( 1+\\sum_q \\left( w_q^p \\right)^2\\right)^2}$\n",
    "dla wszystkich wejść do jednostki $p$. \n",
    "\n",
    "Powoduje to szybsze zanikanie wag dla jednostek, które mają małe wagi wejściowe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostyka i debugowanie algorytmu uczącego\n",
    "\n",
    "Załóżmy, że zaimplementowaliśmy algorytm uczący na określonym (niewielkim) zbiorze danych. Mamy 20 % błędnych decyzji. Co z tym robić dalej?\n",
    "\n",
    "Potencjalnie można:\n",
    "* poprawić algorytm\n",
    "* zdobyć więcej przykładów do ciągu uczącego\n",
    "* wypróbować więcej cech lub ograniczyć przestrzeń cech\n",
    "* pouczyć dłużej albo zmienić algorytm optymalizacyjny\n",
    "* zmienić parametry uczenia (współczynnik szybkości, regularyzacji, itp.)\n",
    "Każda z tych metod naprawia jakiś (ale inny) problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aby zdiagnozować problem dobrze jest analizować wykresy:\n",
    "* błędu od długości uczenia \n",
    "  * czy algorytm jest zbieżny? -> jeśli nie to możemy próbować poprawić to zmniejszając prędkość uczenia lub zmieniając procedurę optymalizacyjną\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* błędu od rozmiaru zbioru uczącego\n",
    "  * Jeśli błąd na zbiorze uczącym dużo niższy niż na testowym to możemy mieć problem przeuczenia\n",
    "    * można: dodać przykładów, zmniejszyć rozmiar wejścia, włączyć regularyzację\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Duży błąd na zbiorze uczącym i na testowym -> źle dobrane cechy lub struktura klasyfikatora \n",
    "    *  zmienić zestaw cech \n",
    "    * wzbogacić strukturę klasyfikatora (np. więcej jednostek ukrytych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bl:  [[ 0.10247073]]\n",
      "bl:  [[ 0.00020618]]\n",
      "bl:  [[  1.62176569e-05]]\n",
      "bl:  [[  5.13608978e-06]]\n",
      "bl:  [[  2.43946393e-06]]\n",
      "bl:  [[  1.40742773e-06]]\n",
      "bl:  [[  9.10272313e-07]]\n",
      "bl:  [[  6.34665251e-07]]\n",
      "bl:  [[  4.66657908e-07]]\n",
      "bl:  [[  3.56972890e-07]]\n",
      "a:  [[ 0.0135618   0.98635568]]\n",
      "y:  [[0 1]]\n",
      "a:  [[ 0.98720792  0.01288262]]\n",
      "y:  [[1 0]]\n",
      "a:  [[ 0.98720719  0.0128826 ]]\n",
      "y:  [[1 0]]\n",
      "a:  [[ 0.01534119  0.98453816]]\n",
      "y:  [[0 1]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "def g(x):\n",
    "    y = 1./(1+np.exp(-x))\n",
    "    return y\n",
    "    \n",
    "def g_prim(x):\n",
    "    y = x*(1-x)\n",
    "    return y\n",
    "    \n",
    "    \n",
    "#zbiór uczący:\n",
    "# wejście, \n",
    "X = np.array([  [0,0],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,1] ])\n",
    "    \n",
    "# wyjście            \n",
    "Y = np.array([[0,1],\n",
    "              [1,0],\n",
    "              [1,0],\n",
    "              [0,1]])\n",
    "            \n",
    "              \n",
    "# definiujemy rozmiary sieci:\n",
    "N_wej = X.shape[1] \n",
    "N_hid = 3\n",
    "N_wyj = Y.shape[1]\n",
    "                            \n",
    "# inicjujemy połączenia\n",
    "# wagi ułożone są tak, że w kolejnych wierszach są kolejne neurony \n",
    "# a w kolumnach wagi od konkretnego neuronu \n",
    "# to +1 jest wagą dla obciążenia\n",
    "w_1 = 2*np.random.random((N_hid, N_wej+1)) - 1# pomiędzy warstwą pierwszą (wejściem) a warstwą ukrytą\n",
    "w_2 = 2*np.random.random((N_wyj, N_hid+1)) - 1\n",
    "\n",
    "                                                \n",
    "for cykl in range(10000):\n",
    "    bl =0\n",
    "    D_1 = np.zeros((N_hid,N_wej+1))\n",
    "    D_2 = np.zeros((N_wyj,N_hid+1))\n",
    "    \n",
    "    \n",
    "    for i in range(0,4):\n",
    "        # weźmy przykład i-ty\n",
    "        \n",
    "        x = X[i,:].reshape(X.shape[1],1)\n",
    "        y = Y[i,:].reshape(Y.shape[1],1)\n",
    "        \n",
    "        # propagacja \"w przód\"\n",
    "        a_0 = np.vstack((1,x))  # z warstwy wejściowej (zerowej) wychodzi a_0\n",
    "        \n",
    "        z_1 = np.dot( w_1, a_0 )# na warstwe 1 wchodzą iloczyny skalarne \n",
    "        a_1 = np.vstack((1,g(z_1))) # dokładamy 1 i dostaję wyjście z warstwy 1\n",
    "        \n",
    "        z_2 = np.dot( w_2, a_1 ) # na warstwe 3 wchodzą iloczyny skalarne \n",
    "        a_2 = g(z_2)\n",
    "        if cykl == 10000-1:\n",
    "            print ('a: ',str(a_2.T))\n",
    "            print ('y: ',str(y.T))\n",
    "        # propagacja \"wstecz\"\n",
    "        d_2 = (a_2 - y)*g_prim(a_2)\n",
    "        d_1 = np.dot(w_2.T, d_2) * g_prim(a_1)#z_2\n",
    "        \n",
    "        # akumulujemy poprawki \n",
    "        D_2 +=  np.dot( d_2, a_1.T)\n",
    "        D_1 +=  np.dot( d_1[1:], a_0.T)\n",
    "        \n",
    "        bl += np.dot(d_2.T,d_2)\n",
    "        \n",
    "    eta1 = 0.6\n",
    "    # uaktualniamy wagi\n",
    "    w_1 -=  eta1*D_1 \n",
    "    w_2 -=  eta1*D_2\n",
    "    \n",
    "    # wypisujemy info o błędzie\n",
    "    if (cykl% 1000) == 0:\n",
    "        print( 'bl: ', bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "progress": true,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "width": 1600
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
