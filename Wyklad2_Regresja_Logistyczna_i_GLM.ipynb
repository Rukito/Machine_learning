{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Powtórzenie:\n",
    "\n",
    "* Co to jest ciąg uczący?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Co to jest hipoteza?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Co to jest funkcja kosztu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Co to jest funkcja wiarygodności?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wstęp\n",
    "* W tym wykładzie zajmiemy się problemem konstrukcji Uogulnionych Modeli Liniowych (ang. ''generalized linear models'' - GML). \n",
    "* Metodologia ta pozwala objąć w jednym formaliźmie zarówno problemy regresji jak i klasyfikacji. \n",
    "* W pewnym sensie klasyfikacja jest podobna do regresji, z tą różnicą, że zmienne które chcemy przewidywać mogą przybierać tylko niewielką ilość dyskretnych wartości. \n",
    "*  Zaczniemy od problemu klasyfikacji binarnej, czyli takiej, w której wejściom mamy przypisywać jedną z dwóch klas, np. oznaczonych 0 i 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regresja logistyczna\n",
    "## Problem \n",
    "* Dane należą do jednej z dóch klas (tzn. `y` w ciągu uczącym jest 0 lub 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5  1.   1.5  2.   2.5]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(-3,3,0.5)\n",
    "y = np.zeros(len(x))\n",
    "y[6:] =1\n",
    "print(x)\n",
    "print(y)\n",
    "py.plot(x[y==0],y[y==0],'ro')\n",
    "py.plot(x[y==1],y[y==1],'bo')\n",
    "py.ylim((-0.1, 1.1))\n",
    "py.xlabel(\"x\")\n",
    "py.ylabel(\"klasa\")\n",
    "py.title(\"Prosty, jednowymiarowy przypadek podziału na klasy\")\n",
    "py.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-3,3,0.25)\n",
    "y = np.zeros(len(x))\n",
    "y[8:] =1\n",
    "y[5] = 1\n",
    "y[10] = 0\n",
    "\n",
    "py.plot(x[y==0],y[y==0],'ro')\n",
    "py.plot(x[y==1],y[y==1],'bo')\n",
    "py.ylim((-0.1, 1.1))\n",
    "py.xlabel(\"x\")\n",
    "py.ylabel(\"klasa\")\n",
    "py.title(\"Trudniejszy, jednowymiarowy przypadek podziału na klasy\")\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "przykład  0: x = ( 9,19), y = 1 \n",
      "przykład  1: x = (19, 8), y = 0 \n",
      "przykład  2: x = (15, 6), y = 0 \n",
      "przykład  3: x = ( 4,13), y = 1 \n",
      "przykład  4: x = ( 5,13), y = 1 \n",
      "przykład  5: x = ( 9, 6), y = 0 \n",
      "przykład  6: x = ( 7, 5), y = 1 \n",
      "przykład  7: x = (12, 0), y = 0 \n",
      "przykład  8: x = ( 8,10), y = 1 \n",
      "przykład  9: x = ( 9,14), y = 1 \n",
      "przykład 10: x = ( 7, 1), y = 0 \n",
      "przykład 11: x = ( 8, 2), y = 0 \n",
      "przykład 12: x = (19,13), y = 1 \n",
      "przykład 13: x = (10, 4), y = 0 \n",
      "przykład 14: x = (12, 1), y = 0 \n",
      "przykład 15: x = (17, 9), y = 0 \n",
      "przykład 16: x = (18, 5), y = 0 \n",
      "przykład 17: x = (10, 7), y = 1 \n",
      "przykład 18: x = ( 7, 6), y = 1 \n",
      "przykład 19: x = ( 2,13), y = 1 \n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(20,size=(20,2))\n",
    "theta = np.array([[-0.2],[0.3]])\n",
    "Y = np.round( 1/(1+np.exp(np.dot(-X, theta))))\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    print('przykład %2d: x = (%2d,%2d), y = %d '%(i, X[i,0],X[i,1], Y[i]))\n",
    "\n",
    "ind0 = np.where(Y==0)\n",
    "ind1 = np.where(Y==1)\n",
    "py.plot(X[ind0,0],X[ind0,1],'bo') \n",
    "py.plot(X[ind1,0],X[ind1,1],'ro')\n",
    "py.xlabel(\"x_0\")\n",
    "py.ylabel(\"x_1\")\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hipoteza\n",
    "* Na chwilę zapomiamy, że zbiór wartości jest dyskretny. \n",
    "* Na hipotezę wybieramy:\n",
    "\n",
    "$g(s) = \\frac{1}{1+ \\exp(-s)}$\n",
    "\n",
    "która wraz z parametrami $\\theta$ i wejściami $x$ jest postaci:\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^T x) =  \\frac{1}{1+ \\exp(-\\theta^T x)}$\n",
    "\n",
    "Pod koniec wykładu okaże się dlaczego taki akurat wybór hipotezy jest bardzo naturalny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jak wygląda funkcja sigmoidalna?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s = np.arange(-5,5,0.01)\n",
    "h = 1/(1 + np.exp(-s))\n",
    "py.plot(s,h)\n",
    "py.xlabel(\"s\")\n",
    "py.ylabel(\"h\")\n",
    "py.title(\"sigmoida\")\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Zwróćmy uwagę na zbiór wartości \n",
    "* i na gradient tej funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "py.plot(s[1:],np.diff(h)/np.diff(s), s, h)\n",
    "py.legend((\"pochodna sigmoidy\",\"sigmoida\"),loc='upper left')\n",
    "\n",
    "\n",
    "py.xlabel(\"s\")\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz:\n",
    "Jaką hipotezę wybraliśmy dla regresji logistycznej?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estymacja parametrów\n",
    "* Jak znaleźć parametry $\\theta$?\n",
    "* W języku funkcji kosztu  moglibyśmy oczywiście zapostulować odpowiednią funkcję kosztu i zastosować do niej minimalizację gradientową. \n",
    "* Poniżej pokażemy, że algorytm można też wyprowadzić z interpretacji probabilistycznej. \n",
    "* Dzięki temu będziemy mogli nabrać nowego wglądu w proces doboru parametrów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ponieważ sigmoida ma zbiór wartości $(0,1)$ to można jej wartość traktować jako prawdopodobieństwo jednej z klas (np. klasa $y=1$):\n",
    "\n",
    "* $P(y=1|x;\\theta) = h_\\theta(x)  $\n",
    "\n",
    "Zauważmy, że wtedy:\n",
    "\n",
    "* $P(y=0|x;\\theta) = 1- h_\\theta(x) $\n",
    "\n",
    "Zauważmy ponadto, że powyższe wyrażenia można zapisać w zwartej formie:\n",
    "$ P(y|x;\\theta) = \\left(h_\\theta(x)\\right)^y \\left(1-h_\\theta(x)\\right)^{1-y}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Funkcja wiarygodności\n",
    "Zakładając, że przykłady zbioru uczącego są niezależne od siebie prawdopodobieństwo zaobserwowania całego zbioru uczącego \n",
    "\n",
    "${\\left(X^{(j)},Y^{(j)}\\right)}_{j=1,\\dots,m}$ wynosi:\n",
    "\n",
    "$P(Y|X;\\theta) = \\prod_{j=1}^m P(y^{(j)}|x^{(j)};\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Możemy to prawdopodobieństwo potraktować jako funkcję parametrów $\\theta$, nazywamy ją wówczas _funkcją wiarygodności_  i oznaczamy $L(\\theta)$.\n",
    "\n",
    "$L(\\theta)= \\prod_{j=1}^m P(y^{(j)}|x^{(j)};\\theta) = \n",
    "\\prod_{j=1}^m \\left(h_\\theta(x^{(j)})\\right)\n",
    "^{y^{(j)}} \\left(1-h_\\theta(x^{(j)})\\right)^{1-y^{(j)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funkcja log-wiarygodności\n",
    "Łatwiejsza w posługiwaniu się jest funkcja log-wiarygodności:\n",
    "\n",
    "$l(\\theta) = \\log L(\\theta) = \\sum_{j=1}^m y^{(j)} \\log h_{\\theta}(x^{(j)}) + (1 - y^{(j)}) \\log (1 - h_{\\theta}(x^{(j)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "Co to jest funkcja wiarygodności?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zasada największej wiarygodności\n",
    "**Dobre parametry $\\theta$ to te, dla których zaobserwowanie ciągu uczącego jest największe.** Aby je znaleźć należy zmaksymalizować funkcję wiarygodności, czy też dowolną monotonicznie rosnącą funkcję funkcji wiarygodności np. log-wiarygodność. Robimy to modyfikując parametry zgodnie z jej pochodną:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$l(\\theta) = \\log L(\\theta) = \\sum_{j=1}^m y^{(j)} \\log h_{\\theta}(x^{(j)}) + (1 - y^{(j)}) \\log (1 - h_{\\theta}(x^{(j)}))$\n",
    "\n",
    "$\n",
    "\\begin{array}{lcl}\n",
    "\\frac{\\partial}{\\partial \\theta_i} l(\\theta) &=&  \\sum_{j=1}^m\\left(y^{(j)}\\frac{1}{g(\\theta^T x^{(j)})} - (1-y^{(j)})\\frac{1}{1-g(\\theta^Tx^{(j)})} \\right) \\frac{\\partial}{\\partial \\theta_i} g(\\theta^T x^{(j)})\\\\\n",
    "&=& \\sum_{j=1}^m \\left(y^{(j)}\\frac{1}{g(\\theta^T x^{(j)})} - (1-y^{(j)})\\frac{1}{1-g(\\theta^Tx^{(j)})} \\right) g(\\theta^T x^{(j)})(1-g(\\theta^T x^{(j)})) \\frac{\\partial}{\\partial \\theta_i} (\\theta^T x^{(j)})\\\\\n",
    "&=& \\sum_{j=1}^m \\left( y^{(j)} (1-g(\\theta^T x^{(j)})) - (1-y^{(j)})g(\\theta^T x^{(j)})\\right)x_i^{(j)}\\\\\n",
    "&=& \\sum_{j=1}^m (y^{(j)}-h_\\theta(x^{(j)}))x_i^{(j)}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "skorzystaliśmy po drodze z postaci pochodnej funkcji logistycznej.\n",
    "\n",
    "Zatem aby zwiększać funkcję wiarygodności powinniśmy parametry zmieniać zgodnie z obliczoną pochodną:\n",
    "\n",
    "$\\theta_i^{(j+1)} :=\\theta_i^{(j)} \n",
    " + \\alpha \\sum_{j=1}^m (y^{(j)} - h_\\theta( x^{(j)}) )x_i^{(j)} $\n",
    "\n",
    "czyli: \n",
    "\n",
    "$\\theta_i^{(j+1)} :=\\theta_i^{(j)} \n",
    " - \\alpha \\sum_{j=1}^m (h_\\theta( x^{(j)}) -y^{(j)} )x_i^{(j)} $\n",
    " \n",
    "Może to się wydać dziwne, ale startując z zupełnie innych założeń i stosując optymalizację innej funkcji dostaliśmy taką samą regułę zmiany parametrów jak przy gradientowej minimalizacji funkcji (średniokwadratowej) kosztu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hiperpowierzchnia podejmowania decyzji w regresji logistycznej\n",
    "\n",
    "Ma ona równanie \n",
    "\n",
    "$\\qquad$ $h_\\theta(x)=1/2$, \n",
    "\n",
    "tzn:\n",
    "\n",
    "$\\qquad$ $\\theta^T x = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Przykładowo dla dwuwymiarowej przestrzeni cech:\n",
    "\n",
    "$\\theta_0 +\\theta_1 x_1 + \\theta_2 x_2 =0 $\n",
    "\n",
    "Przekształcając to do równania prostej we współrzędnych $(x_1,x_2)$ mamy:\n",
    "\n",
    "$- \\theta_2 x_2 = \\theta_0 +\\theta_1 x_1 $\n",
    "\n",
    "$ x_2 = - \\frac{1}{\\theta_2}( \\theta_0 +\\theta_1 x_1 )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rodzina rozkładów wykładniczych  a uogólnione modele liniowe \n",
    "Załóżmy, że chcemy zbudować model służący do szacowania  liczby $y$ klientów odwiedzających sklep (lub witrynę) w dowolnej godzinie, na podstawie pewnych cech $x$, takich jak promocje, ostatnie reklamy, prognoza pogody, dzień tygodnia, itd. \n",
    "\n",
    "Wiemy, że rozkład Poissona zwykle daje dobry model zliczeń np. liczby odwiedzających. \n",
    "\n",
    "Wiedząc o tym, jak możemy wymyślić model dla naszego problemu? \n",
    "\n",
    "Na szczęście, rozkład Poissona należy do *rodziny rozkładów wykładniczych*, więc możemy zastosować uogólniony model liniowy (GLM). \n",
    "\n",
    "W tej sekcji opiszemy metodę konstruowania modeli GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rodzina wykładnicza\n",
    "Mówimy, że dany rozkład należy do rodziny wykładniczej jeśli da się go zapisać w postaci:\n",
    "\n",
    "$\\qquad$$ p(y;\\eta) = b(y) \\exp(\\eta^T T(y) - a(\\eta))$ (*)\n",
    "\n",
    "tutaj:\n",
    "* $\\eta$ nazywana jest parametrem naturalnym lub kanonicznym dystrybucji; \n",
    "* $T(y)$ jest tzw. statystyką wystarczającą (często $T(y) = y$); \n",
    "*  wielkość $\\exp(-a(\\eta))$ jest czynnikiem normalizującym, takim aby rozkład $p(y;\\eta)$ sumował/całkował się do 1.\n",
    "Tak więc członka rodziny wykładniczej określamy podając konkretne postaci $\\eta,\\, T(y),\\, a(\\eta)\\, \\text{oraz} \\, b(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uogólnione modele liniowe\n",
    "Dotychczas rozważaliśmy przykłady regresji gdzie zwykle \n",
    "\n",
    "$\\qquad$$y \\in \\mathcal{N}(\\mu,\\sigma^2)$ \n",
    "\n",
    "oraz klasyfikacji gdzie \n",
    "\n",
    "$\\qquad$$y \\in \\text{Bernoulli}(\\phi)$.\n",
    "\n",
    "\n",
    "Wkrótce przekonamy się, że oba te problemy sa szczególnymi przypadkami większej rodziny modeli, tzw. uogólnionych modeli liniowych.\n",
    "\n",
    "### Uwaga: \n",
    "* W podejściu probabilistycznym myślimy sobie, że zmienna zależna $y$ pochodzi z jakiegoś rozkładu prawdopodobieństwa.\n",
    "* Będziemy często mówić, że **modelujemy** zmienną $y$ za pomocą rozkładu ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozkład Gaussa \n",
    "Rozważymy rozkład  Gaussa. Do estymacji parametrów regresji liniowej nie musieliśmy używać jego wariancji, więc dla uproszczenia obliczeń przyjmiemy, że nasz rozkład Gaussa ma wariancję $\\sigma^2 = 1$.\n",
    "Mamy:\n",
    "\n",
    "$ \n",
    "\\begin{array}{lcl}\n",
    "p(y;\\mu) &=& \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( -\\frac{1}{2} (y-\\mu)^2\\right)\\\\\n",
    "&=& \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{1}{2}y^2 \\right) \\exp\\left(\\mu y - \\frac{1}{2}\\mu^2 \\right)\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "Widzimy więc, że rozkład Gaussa należy do rodziny wykładniczej z następującymi parametrami:\n",
    "\n",
    "$\n",
    "\\begin{array}{lcl}\n",
    "\\eta &=& \\mu \\\\\n",
    "T(y) &=& y \\\\\n",
    "a(\\eta) &=& \\mu^2/2 = \\eta^2/2 \\\\\n",
    "b(y) &=& \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( -\\frac{1}{2} y^2\\right)\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozkład Bernouliego \n",
    "Pokażemy teraz, że rozkład Bernouliego należy do rodziny wykładniczej. \n",
    "\n",
    "Przypomnijmy, że: $\\text{Bernouli}(\\phi)$ to taki rozkład wartości $y \\in \\{0,1\\}$, że:\n",
    "* $p(y=1;\\phi) = \\phi$ zaś \n",
    "* $p(y=0;\\phi) = 1-\\phi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W sposób zwarty możemy napisać ten rozkład tak:\n",
    "\n",
    "$ \n",
    "\\begin{array}{lcl}\n",
    "p(y;\\phi) &=& \\phi^y(1-\\phi)^{1-y} \\\\\n",
    "&=& \\exp(y \\log \\phi + (1-y) \\log (1-\\phi))\\\\\n",
    "&=& \\exp\\left( y \\log \\frac{\\phi}{1-\\phi} + \\log(1-\\phi)\\right)\n",
    "\\end{array}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem w rozkładzie Bernouliego parametrem naturalnym jest\n",
    "\n",
    "$ \\eta = \\log \\frac{\\phi}{1-\\phi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Warto zauważyć, że jeśli przekształcić to wyrażenie ze względu na $\\phi$ to dostaniemy dobrzez znaną funkcję logistyczną: \n",
    "\n",
    "$\\qquad$$\\phi = \\frac{1}{1+\\exp(-\\eta)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Widzimy też, że:\n",
    "* $T(y) = y$\n",
    "* $a(\\eta) = -\\log(1-\\phi) = \\log (1+exp(-\\eta))$\n",
    "* $b(y) = 1$\n",
    "\n",
    "Czyli możemy przedstawić ten rozkład w postaci (*).\n",
    "\n",
    "Rodzina wykładnicza jest znacznie bogatsza. Zawiera w sobie rozkłady wielorakie, Poissona (do modelowania zliczeń), gamma i wykładnicze (np. interwałów czasowych) i wiele innych. W kolejnej sekcji podamy ogólny sposób na konstruowanie modeli, w których $y$ pochodzi z rozkładów wykładniczych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "Jak pokazać, że dany rozkład należy do rodziny rozkładów wykładniczych?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad$$ p(y;\\eta) = b(y) \\exp(\\eta^T T(y) - a(\\eta))$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Konstrukcja uogólnionego modelu liniowego\n",
    "W ogólności chcielibyśmy przewidywać wartość zmiennej losowej  $y$ traktując ją jako funkcję zmiennej $x$. Aby móc zastosować tu metodologię GLM musimy poczynić następujące założenia:\n",
    "* Poszukiwana funkcja zależy od parametrów $\\theta$\n",
    "\n",
    "1) Zmienna $y$ przy ustalonych $x$  i $\\theta$ podlega pewnemu rozkładowi wykładniczemu z parametrem $\\eta$ tzn.:\n",
    "\n",
    "$\\qquad$$(y|x;\\theta) \\sim$ RodzinaWykładnicza($\\eta$). \n",
    "\n",
    "2) Postulujemy, że hipoteza ma postać:\n",
    "\n",
    "$\\qquad$ $h(x)= E[T(y)|x]$\n",
    "* często $T(y) = y$, wtedy  $h(x)= E[y|x]$\n",
    "\n",
    "3) Parametr naturalny $\\eta$ jest **liniowo związany z wejściem** $x$: \n",
    "\n",
    "$\\qquad$$\\eta = \\theta^T x$.\n",
    "\n",
    "Te trzy założenia pozwalają wyprowadzić klasę algorytmów uczących opartych o GLM. Poniżej przedstawimy trzy przykłady."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hipoteza dla regresji liniowej z GLM\n",
    "* Przekonajmy się, że regresja liniowa jest szczególnym przykładem GLM.\n",
    "* Zmienna zależna $y$ jest ciągła i jej prawdopodobieństwo warunkowe dla danego $x$ jest modelowane przez rozkład Gaussa $N(\\mu,\\sigma)$ ($\\mu$ może zależeć od $x$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tak więc w tym wypadku wspomnianą w założeniu 1) RodzinąWykładniczą jest rozkład Gaussa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tak jak widzieliśmy  w tym przypadku $\\eta = \\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dalej mamy:\n",
    "\n",
    "$\n",
    "\\begin{array}{lcl}\n",
    "h_\\theta(x) &=& E[y|x;\\theta]\\\\\n",
    "&=& \\mu \\\\\n",
    "&=& \\eta \\\\\n",
    "&=& \\theta^T x\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "* pierwsza równość wynika z założenia 2, \n",
    "* druga równość wynika z tego, że $(y|x;\\theta) \\sim N(\\mu,\\sigma^2)$, tak więc wartość oczekiwana wynosi $\\mu$\n",
    "* trzecia równość wynika z założenia 1\n",
    "* ostatnia równość wynika  z założenia 3\n",
    "\n",
    "Zauważmy jak przyjęcie założeń co do postaci rozkładu zmiennej zależnej i metodologi GLM (trzy założenia) prowadzą do konkretnej postaci hipotezy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hipoteza dla regresji logistyczna z GLM\n",
    "* Interesuje nas tutaj klasyfikacja binarna, więc $y \\in \\{0,1\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ponieważ $y$ przyjmuje wartości binarne to naturalnym rozkładem prawdopodobieństwa do modelowania warunkowego rozkładu $(y|x)$ jest rozkład Bernoulliego z parametrem $\\phi$ \n",
    "($\\phi$ jest prawdopodbieństwem tego, że _y_=1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Rozkład ten należy do rodziny RozkładówWykładniczych "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dalej zauważmy, że jeśli $(y|x;\\theta) \\sim \\text{Bernoulli}(\\phi)$, to $E[y|x;\\theta] = \\phi$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\phi = \\frac{1}{1+\\exp(-\\eta)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "więc analogicznie jak dla regresji liniowej mamy:\n",
    "$\\qquad$$\n",
    "\\begin{array}{lcl}\n",
    "h_\\theta(x) &=& E[y|x;\\theta]\\\\\n",
    "&=& \\phi \\\\\n",
    "&=& \\frac{1}{1+\\exp(-\\eta)} \\\\\n",
    "&=& \\frac{1}{1+\\exp(-\\theta^T x)}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otrzymaliśmy więc funkcję hipotezy w postaci \n",
    "\n",
    "$\\qquad$$h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^T x)}$. \n",
    "\n",
    "To jest wytłumaczenie dlaczego stosuje się funkcje logistyczne w problemach klasyfikacji: jak tylko założymy, że zmienna zależna podlega rozkładowi Bernoulliego to funkcja logistyczna jest konsekwencją definicji uogólnionych modeli liniowych i rodziny rozkładów wykładniczych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresja wieloraka (softmax)\n",
    "\n",
    "Problem polega na przydzieleniu zmiennych niezależnych do jednej z _k_ klas, czyli zmienna zależna nadal jest dyskretna, ale może przyjmować jedną z _k_ wartości: $y \\in \\{1,2,\\dots,k\\}$. Mówimy, że zmienne _y_ podlegają rozkładowi **wielorakiemu** (ang. _multinomial_).\n",
    "\n",
    "### Przykład 1\n",
    "\n",
    "Rozpoznawanie ręcznie pisanych cyfr: na wejściu mamy obrazek (pixle -> wektor liczb odpowiadających skali szarości danego miejsca), na wyjściu chcemy jedną z klas 0, 1, ..., 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład 2\n",
    "\n",
    "![Klasyczny zbiór danych o irysach: ](Iris_dataset_scatterplot.svg.png \"Klasyczny zbiór danych o irysach\")\n",
    "> Autor: Nicoguaro (Praca własna) [CC BY 4.0 (http://creativecommons.org/licenses/by/4.0)], Wikimedia Commons]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozkład wieloraki należy do rodziny rozkładów wykładniczych\n",
    "* Wyprowadzimy teraz metodę modelowania takich wielorakich danych. \n",
    "* Zaczniemy od wyrażenia rozkładu wielorakiego jako rozkładu należącego do rodziny rozkładów wykładniczych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Parametryzacja\n",
    "* Aby sparametryzować wielorakość z _k_ możliwymi wynikami, można by zacząć od _k_ parametrów $\\phi_1,\\dots,\\phi_k$ określających prawdopodobieństwo każdego z wyników. \n",
    "* Taka parametryzacja jest redundantna, tzn. parametry te nie są niezależne \n",
    "\n",
    "> Dlaczego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* bo znając $k-1$ spośród $\\phi_i$ parametrów ostatni, $k$-ty, parametr jest jednoznacznie określony bo musi być spełniona równość $\\sum_{i=1}^k \\phi_i =1$.) Tak więc sparametryzujemy rozkład przez $k-1$ parametrów:\n",
    "\n",
    "$\\qquad$ $\\phi_1,\\dots,\\phi_{k-1}$, \n",
    "\n",
    "gdzie:\n",
    "* $\\phi_i = p(y=i;\\phi)$ \n",
    "* $p(y=k;\\phi) = 1 - \\sum_{i=1}^{k-1} \\phi_i$. \n",
    "\n",
    "Dla wygody notacji zapiszemy, że \n",
    "\n",
    "$\\qquad$ $\\phi_k = 1-\\sum_{i=1}^{k-1} \\phi_i$, \n",
    "\n",
    "ale będziemy pamiętać, że to nie jest parametr, i że nasz rozkład wieloraki jest w pełni określony przez parametry: \n",
    "\n",
    "$\\qquad$ $\\phi_1,\\dots,\\phi_{k-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Definicja statystyki wystarczającej: $T(y)$\n",
    "Aby wyrazić rozkład wieloraki w języku rodziny rozkładów wykładniczych zdefiniujmy $T(y) \\in \\mathcal{R}^{k-1}$ w następujący sposób:\n",
    "\n",
    "$\n",
    "T(1) = \\left[\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    " \\right],\n",
    "T(2) = \\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    " \\right],\\dots,\n",
    "T(k-1) = \\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{array}\n",
    " \\right], \n",
    "T(k) = \\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    " \\right]\n",
    "$\n",
    "\n",
    "W odróżnieniu od poprzednich przykładów **nie mamy** tu $T(y) = y$, ale $T(y)$ **jest $k-1$ wymiarowym wektorem** a nie skalarem. Aby oznaczyć $i$-ty element tego wektora będziemy pisać $(T(y))_i$. \n",
    "\n",
    "I jeszcze jedna użyteczna konwencja. Wprowadźmy funkcję $1\\{\\cdot\\}$, przyjmuje ona wartość 1 gdy jej argument jest prawdziwy i 0 gdy jest fałszywy, np.: $1\\{2==3\\} = 0$ zaś $1\\{3==7-4\\} = 1$.\n",
    "\n",
    "* Tak więc $(T(y))_i = 1\\{y==i\\}$. \n",
    "* Dalej mamy $E[(T(y))_i] = P(y=i) = \\phi_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zauważmy, że notacja $1\\{ a==b\\}$ jest bardzo naturalna dla `pythona`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(2==3)\n",
    "print(3==7-4)\n",
    "\n",
    "print(np.sum(2==3))\n",
    "print(np.sum(3==7-4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Na funkcję $T(y)$ możemy też patrzeć jak na funkcję wykonującą specyficzne kodowanie przynależności do klasy.\n",
    "* Klasa  $k$ kodowana jest przez wektor samych zer, a każda inna przez umieszczenie $1$ na współrzędnej odpowiadającej numerowi klasy. \n",
    "* Możemy sobie wyobrazić, że taka funkcja steruje diodami, aby poinformować nas o wyniku podłączonego do niej klasyfikatora:\n",
    "![alt wskaznik](wskaznik.jpg \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sprowadzamy rozkład wieloraki do postaci  rozkładu wykładniczego\n",
    "\n",
    "$p(y;\\phi) = \\phi_1^{1\\{y==1\\}}\\phi_2^{1\\{y==2\\}} \\dots \\phi_k^{1\\{y==k\\}}$  $\\qquad$ Tu funkcję $1\\{\\cdot\\}$ zastosowaliśmy jako \"włącznik\" dla konkretnego $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad = \\phi_1^{1\\{y==1\\}}\\phi_2^{1\\{y==2\\}} \\dots \\phi_k^{1-\\sum_{i=1}^{k-1}1\\{y==i\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad = \\phi_1^{(T(y))_1}\\phi_2^{(T(y))_2}\\dots\\phi_k^{1-\\sum_{i=1}^{k-1}(T(y))_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad = \\exp\\left[ (T(y)_1\\log(\\phi_1)) + (T(y)_2\\log(\\phi_2)) + \\dots + (1-\\sum_{i=1}^{k-1} (T(y))_i)\\log(\\phi_k)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad = \\exp \\left[ (T(y))_1 \\log \\frac{\\phi_1}{\\phi_k} + (T(y))_2 \\log \\frac{\\phi_2}{\\phi_k} + \\dots + (T(y))_{k-1} \\log \\frac{\\phi_{k-1}}{\\phi_k} + \\log(\\phi_k)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad = b(y) \\exp(\\eta^T T(y) - a(\\eta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "gdzie:\n",
    "\n",
    "$\n",
    "\\begin{array}{lcl}\n",
    "   \\eta &=& \n",
    "       \\left[\n",
    "            \\begin{array}{c}\n",
    "                \\log \\frac{\\phi_1}{\\phi_k}\\\\\n",
    "                \\log \\frac{\\phi_2}{\\phi_k}\\\\\n",
    "                \\vdots \\\\\n",
    "                \\log \\frac{\\phi_{k-1}}{\\phi_k}\n",
    "            \\end{array} \n",
    "      \\right]\\\\\n",
    "    a(\\eta)&=& -\\log(\\phi_k) \\\\\n",
    "    b(y) &=& 1\n",
    "\\end{array} \n",
    "$\n",
    "\n",
    "To kończy prezentację rozkładu wielorakiego jako członka rodziny rozkładów wykładniczych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Hipoteza dla rozkładu wielorakiego z GLM.\n",
    "Funkcja łącząca (dla $i = 1,\\dots,k$) dana jest przez:\n",
    "\n",
    "$\\qquad$$\\eta_i = \\log \\frac{\\phi_i}{\\phi_k}$\n",
    "dla wygody zdefiniowaliśmy także $\\eta_k = \\log \\frac{\\phi_k}{\\phi_k}$\n",
    "Stąd mamy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad$$\\exp(\\eta_i) = \\frac{\\phi_i}{\\phi_k}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad$$\\phi_k \\exp(\\eta_i) = \\phi_i$$\\qquad$(**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suma po wszystkich możliwych zdarzeniach daje zdarzenie pewne:\n",
    "\n",
    "$\\qquad$$\\phi_k \\sum_{i=1}^{k} \\exp(\\eta_i) = \\sum_{i=1}^k \\phi_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stąd:\n",
    "\n",
    "$\\qquad$$\\phi_k = \\frac{1}{ \\sum_{i=1}^{k} \\exp(\\eta_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Podstawiając to wyrażenie do (**) otrzymujemy funkcję odpowiedzi postaci:\n",
    "\n",
    "$\\qquad$$\\phi_i = \\frac{\\exp(\\eta_i)}{\\sum_{j=1}^k \\exp(\\eta_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ta funkcja mapująca $\\eta$ na $\\phi$ nazywa się funkcją **softmax**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aby dokończyć formulację modelu użyjemy założenia 3, że $\\eta$ jest liniowo związana ze zmienną niezależną $x$. Tak więc mamy:\n",
    "\n",
    "$\\qquad$$\\eta_i = \\theta_i^T x$ dla $i = 1, \\dots,k-1$ gdzie $\\theta$ to parametry modelu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dla wygody notacji definiujemy $\\theta_k = 0$. Wynika stąd, że $\\eta_k = \\theta_k^T x = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem model nasz zakłada, że rozkład warunkowy $(y|x)$ dany jest przez:\n",
    "\n",
    "$\\qquad$\n",
    "$\n",
    "\\begin{array}{lcl}\n",
    "p(y=i|x;\\theta) &=& \\phi_i  \\\\\n",
    "&=&\\frac{\\exp(\\eta_i)}{\\sum_{j=1}^k \\exp(\\eta_j)}  \\\\\n",
    "                &=& \\frac{\\exp(\\theta_i^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)}\n",
    "\\end{array}             \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W wyprowadzonym powyżej modelu regresji softmax hipoteza ma postać:\n",
    "\n",
    "$\n",
    "\\begin{array}{lcl}\n",
    "h_\\theta(x) &=& E[T(y)|x;\\theta]\\\\\n",
    "            &=& E \\left[\n",
    "                  \\begin{array}{lcl}\n",
    "                        1\\{y==1\\} &|& \\\\\n",
    "                        1\\{y==2\\} &|& \\\\\n",
    "                         \\vdots   &|& \\\\\n",
    "                        1\\{y==k-1\\} &|&\n",
    "                  \\end{array} x;\\theta\n",
    "                 \\right]\\\\\n",
    "           &=& \\left[\n",
    "                  \\begin{array}{l}\n",
    "                        \\phi_1 \\\\\n",
    "                        \\phi_2 \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\phi_{k-1}                  \n",
    "                  \\end{array} \n",
    "                 \\right]\\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "$\\qquad$$\n",
    "\\begin{array}{lcl}\n",
    "          &=& \\left[\n",
    "                  \\begin{array}{l}\n",
    "                        \\frac {\\exp(\\theta_1^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)}\\\\\n",
    "                        \\frac {\\exp(\\theta_2^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)} \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\frac {\\exp(\\theta_{k-1}^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)}                  \n",
    "                  \\end{array} \n",
    "                 \\right]\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "Wyrażając to słowami: nasza hipoteza zwróci prawdopodobieństwo warunkowe przynależności danego $x$ do każdej z klas $i$:\n",
    "\n",
    "$\\qquad$$p(y=i|x;\\theta) $ dla $i = 1, \\dots,k$, \n",
    "\n",
    "przy czym prawdopodobieństwo przynależności do ostatniej klasy dane jest przez: \n",
    "\n",
    "$\\qquad$$p(y=k|x;\\theta) = 1-\\sum_{j=1}^{k-1} \\phi_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Estymacja parametrów modelu wielorakiego\n",
    "Na koniec zastanówmy się jak estymować parametry tego modelu. Podobnie jak w przypadku regresji liniowej i regresji logistycznej potrzebny jest nam zbiór uczący postaci $\\left\\{(x^{(j)},y^{(j)})\\right\\}_{ j =1,\\dots,m }$. Można ponownie skorzystać z zasady największej wiarygodności i wyznaczyć parametry $\\theta$, które maksymalizują prawdopodobieństwo zaobserwowania całego zbioru uczącego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Funkcja log-wiarygodności ma postać:\n",
    "\n",
    "$\\qquad$$\n",
    "\\begin{array}{lcl}\n",
    "l(\\theta) &=& \\sum_{j=1}^m \\log p(y{(j)}|x^{(j)};\\theta) \\\\\n",
    "&=& \\sum_{j=1}^m \\log \\prod_{i=1}^k \\left( \\frac{\\exp(\\theta_i^T x^{(j)} )}{\\sum_{n=1}^k \\exp (\\theta_n^T x^{(j)})} \\right)^{1\\{y^{(j)}==i\\}} \n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Zasada największej wiarygodności\n",
    "Teraz maksymalizację $l(\\theta)$ można przeprowadzić np. za pomocą algorytmu gradientowego (tzn. zmieniamy iteracyjnie parametry  w kierunku zgodnym z gradientem funkcji log-wiarygodności)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "Jak znaleźć hipotezę dla danych modelowanych przez pewien znany rozkład w oparciu o GLM?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  },
  "livereveal": {
   "progress": true,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "width": 1600
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
