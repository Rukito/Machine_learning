{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja tekstów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "Ten przykład demonstruje jak można poradzić sobie z klasyfikacją tematyczną dokumentów stosując technikę \"worek ze słowami\". \n",
    "\n",
    "O tej technice więcej można przeczytać tu: [http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction]\n",
    "\n",
    "W skrócie: \n",
    "\n",
    "* tekst jest zamieniany na  pojedyncze słowa. \n",
    "* ze słów otrzymanych dla dużej ilości tekstów tworzony jest słownik - wektor możliwych słów \n",
    "* dla konkretnego tekstu zliczana jest ilość wystąpień każdego ze słów\n",
    "* te zliczenia mogą być normalizowane na różne sposoby\n",
    "\n",
    "O metodzie tej mówi się \"worek ze słowami\", bo zaniedbujemy w niej kolejność słów w dokumencie i wszelkie korelacje między ich wzajemnymi pozycjami. \n",
    "\n",
    "Często stosowana wersja normalizacji to tzw. transformacja tf-idf (term frequency - inverse document frequency)[https://en.wikipedia.org/wiki/Tf–idf]. \n",
    "\n",
    "Transformacja ta zasadza się na iloczynie wagi i specyficzności danego słowa. W najprostszym przypadku:\n",
    "* waga danego słowa jest proporcjonalna do częstości występowania słowa w dokumencie\n",
    "* specyficzność słowa może być określona jako odwrotnie proporcjonalna do liczby dokumentów, w których występuje\n",
    "\n",
    "Możliwe są różne wersje funkcji stosowanych w obu  składnikach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane\n",
    "Dane pochodzą z 20 list dyskusyjnych.\n",
    "Jako klasyfikator wykorzystamy naiwny klasyfikator Bayesa dla rozkładów wielorakich: <tt>MultinomialNB</tt>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład oparty na kodzie z: \n",
    "http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "* Authors: \n",
    "    Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n",
    "    Olivier Grisel <olivier.grisel@ensta.org>,\n",
    "    Mathieu Blondel <mathieu@mblondel.org>,\n",
    "    Lars Buitinck <L.J.Buitinck@uva.nl>,\n",
    "* License: BSD 3 clause\n",
    "* Adaptacja: Jarosław Żygierewicz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W całym zbiorze danych jest 20 list dyskusyjnych, tu wykorzystamy podzbiór.\n",
    "\n",
    "Kategorie dla których zbudujemy klasyfikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space'  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładujemy dane z newsgroups dataset dla wybranch kategorii. Od razu przygotowujemy dane testowe i treningowe.\n",
    "Korzystamy z funkcji sklearn.datasets.fetch_20newsgroups http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "categories = data_train.target_names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak wyglądają przykładowe dane o numerze 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id =57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista wiadomości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using Photoshop is there anyway to get an elliptical dot for the\n",
      "halftone screen rather than a round dot ? My printer would prefer an\n",
      "elliptical dot, but I'm not sure how to set it up. I'm sending from a Mac\n",
      "IIci to a Linotronic L300 imagesetter and I am using Photoshop 2.0.1 to\n",
      "make my separations.\n",
      "\n",
      "Any help would be greatly appreshed. T.I.A. \n",
      "\n",
      "Michael (Unscene) \n",
      "\n",
      "\n",
      "Michael Maier, Computer Artist, ANL  |  [|Ú]---*Z* Glued to the veiw.      \n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[id])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista kodów tematycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data_train.target[id])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nazwy kategorii odpowiadających kodom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(categories[data_train.target[id]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upraszczamy nazewnictwo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-572dccfa8b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train, y_test = data_train.target, data_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekodowujemy wiadomości na wekotry cech.  Korzystamy z funkcji: [sklearn.feature_extraction.text.TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = ....# stwórz instancje obiektu TfidfVectorizer\n",
    "X_train = ....# naucz vctorizer słownika i przetransformuj dane uczące.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypisz rozmiary danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Dane treningowe: n_samples: %d, n_features: %d\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane uczące są przechowywane w macierzy rzadkiej (sparse matrix). Proszę podejrzeć jak wyglądają tak przekodowane dane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Wektoryzujemy też dane testowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = ...\n",
    "print(\"Dane testowe: n_samples: %d, n_features: %d\" % X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odwrotne mapowanie z cech na słowa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy instancję i uczymy klasyfikator MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ...\n",
    "clf...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: tu będziemy korzystać z funkcji zaimplementowanych w \n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = clf.... # obliczamy predykcję dla tekstów ze zbioru testowego\n",
    "accur = ... # dokladność\n",
    "print(\"dokładność:   %0.3f\" % accur)\n",
    "print(\"classification report:\") # wypisz raport klasyfikacji \n",
    "print(...)\n",
    "\n",
    "print(\"Macierz błędów\") # wypisz macierz (confusion matrix)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypiszemy teraz po 10 najbardziej znaczących słów w każdej klasie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(u\"top 10 haseł na klasę:\")\n",
    "for i, category in enumerate(categories):\n",
    "    top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "    print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
